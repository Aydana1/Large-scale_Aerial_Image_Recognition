{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0 True\n"
     ]
    }
   ],
   "source": [
    "from detectron2.modeling.meta_arch import GeneralizedRCNN as RCNN\n",
    "from detectron2.modeling.anchor_generator import DefaultAnchorGenerator\n",
    "from detectron2.modeling.proposal_generator.rpn import RPN, StandardRPNHead\n",
    "from detectron2.modeling.backbone.resnet import ResNet, BasicStem, BasicBlock, BottleneckBlock, DeformBottleneckBlock\n",
    "from detectron2.modeling.backbone.fpn import FPN, LastLevelMaxPool\n",
    "from detectron2.modeling.roi_heads import StandardROIHeads, FastRCNNConvFCHead\n",
    "from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputLayers\n",
    "from detectron2.modeling.box_regression import Box2BoxTransform\n",
    "from detectron2.modeling.matcher import Matcher\n",
    "from detectron2.modeling.poolers import ROIPooler\n",
    "from detectron2.solver.build import get_default_optimizer_params\n",
    "from detectron2.data.build import build_batch_data_loader, get_detection_dataset_dicts\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data.common import DatasetFromList, MapDataset\n",
    "from detectron2.data.dataset_mapper import DatasetMapper\n",
    "from detectron2.data.samplers import TrainingSampler\n",
    "from detectron2.layers import ShapeSpec\n",
    "from detectron2.engine.train_loop import SimpleTrainer\n",
    "from detectron2.solver.lr_scheduler import LRMultiplier, WarmupParamScheduler\n",
    "from detectron2.checkpoint.detection_checkpoint import DetectionCheckpointer\n",
    "from detectron2.data import transforms as T\n",
    "from detectron2.data import detection_utils as utils\n",
    "from fvcore.common.param_scheduler import MultiStepParamScheduler\n",
    "from munch import Munch as m\n",
    "\n",
    "import os\n",
    "import weakref\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Munch({'OUTPUT_DIR': 'output_fasterrcnn', 'MODEL': Munch({'PIXEL_MEAN': [103.53, 116.28, 123.675], 'PIXEL_STD': [1.0, 1.0, 1.0], 'MASK_ON': False, 'KEYPOINT_ON': False, 'LOAD_PROPOSALS': False, 'WEIGHTS': 'https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_101_FPN_3x/137851257/model_final_f6e8b1.pkl', 'BACKBONE': Munch({'FREEZE_AT': 2}), 'RESNETS': Munch({'OUT_FEATURES': ['res2', 'res3', 'res4', 'res5'], 'STEM_OUT_CHANNELS': 64, 'DEPTH': 101, 'NUM_GROUPS': 1, 'WIDTH_PER_GROUP': 64, 'RES2_OUT_CHANNELS': 256, 'STRIDE_IN_1X1': True, 'RES5_DILATION': 1, 'DEFORM_ON_PER_STAGE': [False, False, False, False], 'DEFORM_MODULATED': False, 'DEFORM_NUM_GROUPS': 1, 'NORM': 'FrozenBN'}), 'ANCHOR_GENERATOR': Munch({'SIZES': [[32], [64], [128], [256], [512]], 'ASPECT_RATIOS': [[0.5, 1.0, 2.0]], 'OFFSET': 0.0}), 'FPN': Munch({'IN_FEATURES': ['res2', 'res3', 'res4', 'res5'], 'OUT_CHANNELS': 256, 'FUSE_TYPE': 'sum', 'NORM': ''}), 'RPN': Munch({'IN_FEATURES': ['p2', 'p3', 'p4', 'p5', 'p6'], 'PRE_NMS_TOPK_TRAIN': 2000, 'PRE_NMS_TOPK_TEST': 1000, 'POST_NMS_TOPK_TRAIN': 1000, 'POST_NMS_TOPK_TEST': 1000, 'SMOOTH_L1_BETA': 0.0, 'BBOX_REG_LOSS_TYPE': 'smooth_l1', 'LOSS_WEIGHT': 1.0, 'BBOX_REG_LOSS_WEIGHT': 1.0, 'BOUNDARY_THRESH': -1, 'NMS_THRESH': 0.7, 'POSITIVE_FRACTION': 0.5, 'BATCH_SIZE_PER_IMAGE': 256, 'BBOX_REG_WEIGHTS': (1.0, 1.0, 1.0, 1.0), 'IOU_LABELS': [0, -1, 1], 'IOU_THRESHOLDS': [0.3, 0.7], 'CONV_DIMS': [-1]}), 'ROI_HEADS': Munch({'IN_FEATURES': ['p2', 'p3', 'p4', 'p5'], 'NUM_CLASSES': 15, 'SCORE_THRESH_TEST': 0.05, 'NMS_THRESH_TEST': 0.5, 'BATCH_SIZE_PER_IMAGE': 128, 'POSITIVE_FRACTION': 0.25, 'IOU_THRESHOLDS': [0.5], 'IOU_LABELS': [0, 1]}), 'ROI_BOX_HEAD': Munch({'NUM_CLASSES': 4, 'NUM_FC': 4, 'FC_DIM': 1024, 'POOLER_RESOLUTION': 14, 'CONV_DIM': 256, 'NUM_CONV': 0, 'NORM': '', 'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0), 'CLS_AGNOSTIC_BBOX_REG': False, 'SMOOTH_L1_BETA': 0.0, 'BBOX_REG_LOSS_TYPE': 'smooth_l1', 'BBOX_REG_LOSS_WEIGHT': 1.0, 'POOLER_SAMPLING_RATIO': 0, 'POOLER_TYPE': 'ROIAlignV2', 'TRAIN_ON_PRED_BOXES': False}), 'ROI_MASK_HEAD': Munch({'NUM_CONV': 2, 'POOLER_RESOLUTION': 7}), 'ROI_KEYPOINT_HEAD': Munch({}), 'PROPOSAL_GENERATOR': Munch({'MIN_SIZE': 0})}), 'SOLVER': Munch({'IMS_PER_BATCH': 2, 'WEIGHT_DECAY_NORM': 0.0, 'BIAS_LR_FACTOR': 1.0, 'WARMUP_FACTOR': 0.001, 'WARMUP_ITERS': 1000, 'WARMUP_METHOD': 'linear', 'WEIGHT_DECAY_BIAS': None, 'MOMENTUM': 0.9, 'NESTEROV': False, 'WEIGHT_DECAY': 0.0001, 'GAMMA': 0.1, 'BASE_LR': 0.02, 'STEPS': (), 'MAX_ITER': 100000, 'CLIP_GRADIENTS': Munch({'ENABLED': False})}), 'INPUT': Munch({'MIN_SIZE_TRAIN': (640, 672, 704, 736, 768, 800), 'MAX_SIZE_TRAIN': 1333, 'MIN_SIZE_TRAIN_SAMPLING': 'choice', 'RANDOM_FLIP': 'horizontal', 'MASK_FORMAT': 'polygon', 'FORMAT': 'BGR', 'CROP': Munch({'ENABLED': False, 'TYPE': 'relative_range', 'SIZE': [0.9, 0.9]})}), 'TEST': Munch({'DETECTIONS_PER_IMAGE': 100}), 'DATALOADER': Munch({'NUM_WORKERS': 2, 'FILTER_EMPTY_ANNOTATIONS': True, 'ASPECT_RATIO_GROUPING': True, 'SAMPLER_TRAIN': 'TrainingSampler'}), 'DATASETS': Munch({'TRAIN': ('iSAID_train',), 'TEST': (), 'PROPOSAL_FILES_TRAIN': ()})})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = m({\n",
    "    'OUTPUT_DIR': 'output_fasterrcnn',\n",
    "    'MODEL': m({\n",
    "        # 'META_ARCHITECTURE': \"GeneralizedRCNN\", # not used in our code\n",
    "        'PIXEL_MEAN': [103.530, 116.280, 123.675],\n",
    "        'PIXEL_STD': [1.0, 1.0, 1.0],\n",
    "        'MASK_ON': False, # not used in our code\n",
    "        'KEYPOINT_ON': False, # not used in our code\n",
    "        'LOAD_PROPOSALS': False, # not used in our code\n",
    "        'WEIGHTS': \"https://dl.fbaipublicfiles.com/detectron2/\"\n",
    "                +\"COCO-Detection/faster_rcnn_R_101_FPN_3x\"\n",
    "                +\"/\"+\"137851257/model_final_f6e8b1.pkl\", # \"COCO-Detection/faster_rcnn_R_101_FPN_3x\" -> \"137851257/model_final_f6e8b1.pkl\",\n",
    "        'MASK_ON': False,\n",
    "        'BACKBONE': m({\n",
    "            # 'NAME': \"build_resnet_fpn_backbone\", # not used in our code\n",
    "            'FREEZE_AT': 2,\n",
    "        }),\n",
    "        'RESNETS': m({\n",
    "            'OUT_FEATURES': [\"res2\", \"res3\", \"res4\", \"res5\"],\n",
    "            'STEM_OUT_CHANNELS': 64,\n",
    "            'DEPTH': 101,\n",
    "            'NUM_GROUPS': 1,\n",
    "            'WIDTH_PER_GROUP': 64,\n",
    "            'RES2_OUT_CHANNELS': 256,\n",
    "            'STRIDE_IN_1X1': True,\n",
    "            'RES5_DILATION': 1,\n",
    "            'DEFORM_ON_PER_STAGE': [False, False, False, False],\n",
    "            'DEFORM_MODULATED': False,\n",
    "            'DEFORM_NUM_GROUPS': 1,\n",
    "            'NORM': 'FrozenBN',\n",
    "        }),\n",
    "        'ANCHOR_GENERATOR': m({\n",
    "            'SIZES': [[32], [64], [128], [256], [512]], # One size for each in feature map\n",
    "            'ASPECT_RATIOS': [[0.5, 1.0, 2.0]],  # Three aspect ratios (same for all in feature maps)\n",
    "            'OFFSET': 0.0,\n",
    "        }),\n",
    "        'FPN': m({\n",
    "            'IN_FEATURES': [\"res2\", \"res3\", \"res4\", \"res5\"],\n",
    "            'OUT_CHANNELS': 256,\n",
    "            'FUSE_TYPE': \"sum\",\n",
    "            'NORM': \"\", #\"GN\",\n",
    "        }),\n",
    "        'RPN': m({\n",
    "            'IN_FEATURES': [\"p2\", \"p3\", \"p4\", \"p5\", \"p6\"],\n",
    "            'PRE_NMS_TOPK_TRAIN': 2000,\n",
    "            'PRE_NMS_TOPK_TEST': 1000,\n",
    "            'POST_NMS_TOPK_TRAIN': 1000,\n",
    "            'POST_NMS_TOPK_TEST': 1000,\n",
    "            'SMOOTH_L1_BETA': 0.0,\n",
    "            'BBOX_REG_LOSS_TYPE': \"smooth_l1\",\n",
    "            'LOSS_WEIGHT': 1.0,\n",
    "            'BBOX_REG_LOSS_WEIGHT': 1.0,\n",
    "            'BOUNDARY_THRESH': -1,\n",
    "            'NMS_THRESH': 0.7,\n",
    "            'POSITIVE_FRACTION': 0.5,\n",
    "            'BATCH_SIZE_PER_IMAGE': 256,\n",
    "            'BBOX_REG_WEIGHTS': (1.0, 1.0, 1.0, 1.0),\n",
    "            'IOU_LABELS': [0, -1, 1],\n",
    "            'IOU_THRESHOLDS': [0.3, 0.7],\n",
    "            'CONV_DIMS': [-1],\n",
    "        }),\n",
    "        'ROI_HEADS': m({\n",
    "            # 'NAME': \"StandardROIHeads\", # not used in our code\n",
    "            'IN_FEATURES': [\"p2\", \"p3\", \"p4\", \"p5\"],\n",
    "            'NUM_CLASSES': 15,\n",
    "            'SCORE_THRESH_TEST': 0.05,\n",
    "            'NMS_THRESH_TEST': 0.5,\n",
    "            'BATCH_SIZE_PER_IMAGE': 128,\n",
    "            'POSITIVE_FRACTION': 0.25,\n",
    "            'IOU_THRESHOLDS': [0.5],\n",
    "            'IOU_LABELS': [0, 1],\n",
    "        }),\n",
    "        'ROI_BOX_HEAD': m({\n",
    "            # 'NAME': \"FastRCNNConvFCHead\", # not used in our code\n",
    "            'NUM_CLASSES': 4,\n",
    "            'NUM_FC': 4,\n",
    "            'FC_DIM': 1024,\n",
    "            'POOLER_RESOLUTION': 14,\n",
    "            'CONV_DIM': 256,\n",
    "            'NUM_CONV': 0,\n",
    "            'NORM': \"\",\n",
    "            'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),\n",
    "            'CLS_AGNOSTIC_BBOX_REG': False,\n",
    "            'SMOOTH_L1_BETA': 0.0,\n",
    "            'BBOX_REG_LOSS_TYPE': \"smooth_l1\",\n",
    "            'BBOX_REG_LOSS_WEIGHT': 1.0,\n",
    "            'POOLER_SAMPLING_RATIO': 0,\n",
    "            'POOLER_TYPE': \"ROIAlignV2\",\n",
    "            'TRAIN_ON_PRED_BOXES': False,\n",
    "        }),\n",
    "        'ROI_MASK_HEAD': m({\n",
    "            # 'NAME': \"MaskRCNNConvUpsampleHead\", # not used in our code\n",
    "            'NUM_CONV': 2,\n",
    "            'POOLER_RESOLUTION': 7,\n",
    "        }),\n",
    "        'ROI_KEYPOINT_HEAD': m({\n",
    "            # 'MIN_KEYPOINTS_PER_IMAGE': \"MaskRCNNConvUpsampleHead\", # not used in our code\n",
    "        }),\n",
    "        'PROPOSAL_GENERATOR': m({\n",
    "            'MIN_SIZE': 0,\n",
    "        }),\n",
    "    }),\n",
    "    'SOLVER': m({\n",
    "        'IMS_PER_BATCH': 2,\n",
    "        'WEIGHT_DECAY_NORM': 0.0,\n",
    "        'BIAS_LR_FACTOR': 1.0,\n",
    "        'WARMUP_FACTOR': 1.0 / 1000,\n",
    "        'WARMUP_ITERS': 1000,\n",
    "        'WARMUP_METHOD': \"linear\",\n",
    "        'WEIGHT_DECAY_BIAS': None,\n",
    "        'MOMENTUM': 0.9,\n",
    "        'NESTEROV': False,\n",
    "        'WEIGHT_DECAY': 0.0001,\n",
    "        'GAMMA': 0.1,\n",
    "        'BASE_LR': 0.02, # 0.00025\n",
    "        'STEPS': (), # (210000, 250000),\n",
    "        'MAX_ITER': 100000,\n",
    "        'CLIP_GRADIENTS': m({\n",
    "            'ENABLED': False, # not used in our code\n",
    "        })\n",
    "    }),\n",
    "    'INPUT': m({\n",
    "        'MIN_SIZE_TRAIN': (640, 672, 704, 736, 768, 800),\n",
    "        'MAX_SIZE_TRAIN': 1333,\n",
    "        'MIN_SIZE_TRAIN_SAMPLING': \"choice\",\n",
    "        'RANDOM_FLIP': \"horizontal\",\n",
    "        'MASK_FORMAT': \"polygon\",\n",
    "        'FORMAT': \"BGR\",\n",
    "        'CROP': m({\n",
    "            'ENABLED': False,\n",
    "            'TYPE': \"relative_range\",\n",
    "            'SIZE': [0.9, 0.9],\n",
    "        }),\n",
    "    }),\n",
    "    'TEST': m({\n",
    "        'DETECTIONS_PER_IMAGE': 100,\n",
    "    }),\n",
    "    'DATALOADER': m({\n",
    "        'NUM_WORKERS': 2,\n",
    "        'FILTER_EMPTY_ANNOTATIONS': True,\n",
    "        'ASPECT_RATIO_GROUPING': True,\n",
    "        'SAMPLER_TRAIN': \"TrainingSampler\", # not used in our code\n",
    "    }),\n",
    "    'DATASETS': m({\n",
    "        'TRAIN': (\"iSAID_train\",),\n",
    "        'TEST': (),\n",
    "        'PROPOSAL_FILES_TRAIN': (),\n",
    "    }),\n",
    "})\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, Define the FPN bottom-up part (which is a ResNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_shape = ShapeSpec(channels=len(config.MODEL.PIXEL_MEAN))\n",
    "norm = config.MODEL.RESNETS.NORM\n",
    "stem = BasicStem(\n",
    "    in_channels     = input_shape.channels,\n",
    "    out_channels    = config.MODEL.RESNETS.STEM_OUT_CHANNELS,\n",
    "    norm            = norm,\n",
    ")\n",
    "\n",
    "freeze_at           = config.MODEL.BACKBONE.FREEZE_AT\n",
    "out_features        = config.MODEL.RESNETS.OUT_FEATURES\n",
    "depth               = config.MODEL.RESNETS.DEPTH\n",
    "num_groups          = config.MODEL.RESNETS.NUM_GROUPS\n",
    "width_per_group     = config.MODEL.RESNETS.WIDTH_PER_GROUP\n",
    "bottleneck_channels = num_groups * width_per_group\n",
    "in_channels         = config.MODEL.RESNETS.STEM_OUT_CHANNELS\n",
    "out_channels        = config.MODEL.RESNETS.RES2_OUT_CHANNELS\n",
    "stride_in_1x1       = config.MODEL.RESNETS.STRIDE_IN_1X1\n",
    "res5_dilation       = config.MODEL.RESNETS.RES5_DILATION\n",
    "deform_on_per_stage = config.MODEL.RESNETS.DEFORM_ON_PER_STAGE\n",
    "deform_modulated    = config.MODEL.RESNETS.DEFORM_MODULATED\n",
    "deform_num_groups   = config.MODEL.RESNETS.DEFORM_NUM_GROUPS\n",
    "\n",
    "# assert res5_dilation in {1, 2}, \"res5_dilation cannot be {}.\".format(res5_dilation)\n",
    "\n",
    "num_blocks_per_stage = {\n",
    "    18: [2, 2, 2, 2],\n",
    "    34: [3, 4, 6, 3],\n",
    "    50: [3, 4, 6, 3],\n",
    "    101: [3, 4, 23, 3],\n",
    "    152: [3, 8, 36, 3],\n",
    "}[depth]\n",
    "\n",
    "if depth in [18, 34]:\n",
    "    assert out_channels == 64, \"Must set MODEL.RESNETS.RES2_OUT_CHANNELS = 64 for R18/R34\"\n",
    "    assert not any(\n",
    "        deform_on_per_stage\n",
    "    ), \"MODEL.RESNETS.DEFORM_ON_PER_STAGE unsupported for R18/R34\"\n",
    "    assert res5_dilation == 1, \"Must set MODEL.RESNETS.RES5_DILATION = 1 for R18/R34\"\n",
    "    assert num_groups == 1, \"Must set MODEL.RESNETS.NUM_GROUPS = 1 for R18/R34\"\n",
    "\n",
    "stages = []\n",
    "\n",
    "for idx, stage_idx in enumerate(range(2, 6)):\n",
    "    # res5_dilation is used this way as a convention in R-FCN & Deformable Conv paper\n",
    "    dilation = res5_dilation if stage_idx == 5 else 1\n",
    "    first_stride = 1 if idx == 0 or (stage_idx == 5 and dilation == 2) else 2\n",
    "    stage_kargs = {\n",
    "        \"num_blocks\": num_blocks_per_stage[idx],\n",
    "        \"stride_per_block\": [first_stride] + [1] * (num_blocks_per_stage[idx] - 1),\n",
    "        \"in_channels\": in_channels,\n",
    "        \"out_channels\": out_channels,\n",
    "        \"norm\": norm,\n",
    "    }\n",
    "    # Use BasicBlock for R18 and R34.\n",
    "    if depth in [18, 34]:\n",
    "        stage_kargs[\"block_class\"] = BasicBlock\n",
    "    else:\n",
    "        stage_kargs[\"bottleneck_channels\"] = bottleneck_channels\n",
    "        stage_kargs[\"stride_in_1x1\"] = stride_in_1x1\n",
    "        stage_kargs[\"dilation\"] = dilation\n",
    "        stage_kargs[\"num_groups\"] = num_groups\n",
    "        if deform_on_per_stage[idx]:\n",
    "            stage_kargs[\"block_class\"] = DeformBottleneckBlock\n",
    "            stage_kargs[\"deform_modulated\"] = deform_modulated\n",
    "            stage_kargs[\"deform_num_groups\"] = deform_num_groups\n",
    "        else:\n",
    "            stage_kargs[\"block_class\"] = BottleneckBlock\n",
    "    blocks = ResNet.make_stage(**stage_kargs)\n",
    "    in_channels = out_channels\n",
    "    out_channels *= 2\n",
    "    bottleneck_channels *= 2\n",
    "    stages.append(blocks)\n",
    "\n",
    "# fpn_bottom_up (type Backbone > nn.Module)\n",
    "fpn_bottom_up = ResNet(stem, stages, out_features=out_features, freeze_at=freeze_at)\n",
    "# fpn_bottom_up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = FPN(\n",
    "    bottom_up       = fpn_bottom_up,\n",
    "    in_features     = config.MODEL.FPN.IN_FEATURES,\n",
    "    out_channels    = config.MODEL.FPN.OUT_CHANNELS,\n",
    "    norm            = config.MODEL.FPN.NORM,\n",
    "    fuse_type       = config.MODEL.FPN.FUSE_TYPE,\n",
    "    top_block       = LastLevelMaxPool(),\n",
    ")\n",
    "# backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Region Proposal Network (RPN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Anchor generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = backbone.output_shape()\n",
    "input_shape = [input_shape[f] for f in config.MODEL.RPN.IN_FEATURES]\n",
    "\n",
    "anchor_generator = DefaultAnchorGenerator(\n",
    "    sizes           = config.MODEL.ANCHOR_GENERATOR.SIZES, \n",
    "    aspect_ratios   = config.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS,\n",
    "    strides         = [x.stride for x in input_shape],\n",
    "    offset          = config.MODEL.ANCHOR_GENERATOR.OFFSET,\n",
    ")\n",
    "# anchor_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the RPN head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = backbone.output_shape()\n",
    "input_shape = [input_shape[f] for f in config.MODEL.RPN.IN_FEATURES]\n",
    "in_channels = [s.channels for s in input_shape]\n",
    "assert len(set(in_channels)) == 1, \"Each level must have the same channel!\"\n",
    "in_channels = in_channels[0]\n",
    "\n",
    "assert (\n",
    "    len(set(anchor_generator.num_anchors)) == 1\n",
    "), \"Each level must have the same number of anchors per spatial position\"\n",
    "\n",
    "rpn_head = StandardRPNHead(\n",
    "    in_channels = in_channels,\n",
    "    num_anchors = anchor_generator.num_anchors[0],\n",
    "    box_dim     = anchor_generator.box_dim,\n",
    "    conv_dims   = config.MODEL.RPN.CONV_DIMS,\n",
    ")\n",
    "# rpn_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Region Proposal Generator (RPN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_generator = RPN(\n",
    "    in_features             = config.MODEL.RPN.IN_FEATURES,\n",
    "    head                    = rpn_head,\n",
    "    anchor_generator        = anchor_generator,\n",
    "    anchor_matcher          = Matcher(\n",
    "                                config.MODEL.RPN.IOU_THRESHOLDS, config.MODEL.RPN.IOU_LABELS, allow_low_quality_matches=True\n",
    "                                ),\n",
    "    box2box_transform       = Box2BoxTransform(weights=config.MODEL.RPN.BBOX_REG_WEIGHTS),\n",
    "    batch_size_per_image    = config.MODEL.RPN.BATCH_SIZE_PER_IMAGE,\n",
    "    positive_fraction       = config.MODEL.RPN.POSITIVE_FRACTION,\n",
    "    pre_nms_topk            = (config.MODEL.RPN.POST_NMS_TOPK_TRAIN, config.MODEL.RPN.POST_NMS_TOPK_TEST),\n",
    "    post_nms_topk           = (config.MODEL.RPN.POST_NMS_TOPK_TRAIN, config.MODEL.RPN.POST_NMS_TOPK_TEST),\n",
    "    nms_thresh              = config.MODEL.RPN.NMS_THRESH,\n",
    "    min_box_size            = config.MODEL.PROPOSAL_GENERATOR.MIN_SIZE,\n",
    "    anchor_boundary_thresh  = config.MODEL.RPN.BOUNDARY_THRESH,\n",
    "    loss_weight             = {\n",
    "                                \"loss_rpn_cls\": config.MODEL.RPN.LOSS_WEIGHT,\n",
    "                                \"loss_rpn_loc\": config.MODEL.RPN.BBOX_REG_LOSS_WEIGHT * config.MODEL.RPN.LOSS_WEIGHT,\n",
    "                                },\n",
    "    box_reg_loss_type       = config.MODEL.RPN.BBOX_REG_LOSS_TYPE,\n",
    "    smooth_l1_beta          = config.MODEL.RPN.SMOOTH_L1_BETA,\n",
    ")\n",
    "# proposal_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Region of Interest Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = backbone.output_shape()\n",
    "input_shape = [input_shape[f] for f in config.MODEL.RPN.IN_FEATURES]\n",
    "in_channels = [s.channels for s in input_shape]\n",
    "assert len(set(in_channels)) == 1, \"Each level must have the same channel!\"\n",
    "in_channels = in_channels[0]\n",
    "\n",
    "box_head = FastRCNNConvFCHead(\n",
    "    input_shape = ShapeSpec(\n",
    "                    channels    = in_channels, \n",
    "                    height      = config.MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION, \n",
    "                    width       = config.MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION\n",
    "                    ),\n",
    "    conv_dims   = [config.MODEL.ROI_BOX_HEAD.CONV_DIM] * config.MODEL.ROI_BOX_HEAD.NUM_CONV,\n",
    "    fc_dims     = [config.MODEL.ROI_BOX_HEAD.FC_DIM] * config.MODEL.ROI_BOX_HEAD.NUM_FC,\n",
    "    conv_norm   = config.MODEL.ROI_BOX_HEAD.NORM,\n",
    ")\n",
    "\n",
    "box_predictor = FastRCNNOutputLayers(\n",
    "        input_shape             = box_head.output_shape,\n",
    "        box2box_transform       = Box2BoxTransform(weights=config.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS),\n",
    "        num_classes             = config.MODEL.ROI_HEADS.NUM_CLASSES,\n",
    "        test_score_thresh       = config.MODEL.ROI_HEADS.SCORE_THRESH_TEST,\n",
    "        test_nms_thresh         = config.MODEL.ROI_HEADS.NMS_THRESH_TEST,\n",
    "        test_topk_per_image     = config.TEST.DETECTIONS_PER_IMAGE,\n",
    "        cls_agnostic_bbox_reg   = config.MODEL.ROI_BOX_HEAD.CLS_AGNOSTIC_BBOX_REG,\n",
    "        smooth_l1_beta          = config.MODEL.ROI_BOX_HEAD.SMOOTH_L1_BETA,\n",
    "        box_reg_loss_type       = config.MODEL.ROI_BOX_HEAD.BBOX_REG_LOSS_TYPE,\n",
    "        loss_weight             = {\"loss_box_reg\": config.MODEL.ROI_BOX_HEAD.BBOX_REG_LOSS_WEIGHT},\n",
    ")\n",
    "# box_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = backbone.output_shape()\n",
    "\n",
    "roi_pooler = ROIPooler(\n",
    "    output_size     = config.MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION,\n",
    "    scales          = tuple(1.0 / input_shape[k].stride for k in config.MODEL.ROI_HEADS.IN_FEATURES),\n",
    "    sampling_ratio  = config.MODEL.ROI_BOX_HEAD.POOLER_SAMPLING_RATIO,\n",
    "    pooler_type     = config.MODEL.ROI_BOX_HEAD.POOLER_TYPE,\n",
    ")\n",
    "\n",
    "roi_heads = StandardROIHeads(\n",
    "    box_in_features         = config.MODEL.ROI_HEADS.IN_FEATURES,\n",
    "    box_pooler              = roi_pooler,\n",
    "    box_head                = box_head,\n",
    "    box_predictor           = box_predictor,\n",
    "    num_classes             = config.MODEL.ROI_HEADS.NUM_CLASSES,\n",
    "    batch_size_per_image    = config.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE,\n",
    "    positive_fraction       = config.MODEL.ROI_HEADS.POSITIVE_FRACTION,\n",
    "    proposal_matcher        = Matcher(\n",
    "                                config.MODEL.ROI_HEADS.IOU_THRESHOLDS,\n",
    "                                config.MODEL.ROI_HEADS.IOU_LABELS,\n",
    "                                allow_low_quality_matches=False,\n",
    "                                ),\n",
    "    proposal_append_gt      = True,\n",
    "    mask_in_features        = None, #optional\n",
    "    mask_pooler             = None, #optional\n",
    "    mask_head               = None, #optional\n",
    "    keypoint_in_features    = None, #optional\n",
    "    keypoint_pooler         = None, #optional\n",
    "    keypoint_head           = None, #optional\n",
    "    train_on_pred_boxes     = config.MODEL.ROI_BOX_HEAD.TRAIN_ON_PRED_BOXES\n",
    ")\n",
    "# roi_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "faster_rcnn_101 = RCNN(\n",
    "    backbone            = backbone,\n",
    "    proposal_generator  = proposal_generator,\n",
    "    roi_heads           = roi_heads,\n",
    "    pixel_mean          = config.MODEL.PIXEL_MEAN,\n",
    "    pixel_std           = config.MODEL.PIXEL_STD,\n",
    "    input_format        = None,\n",
    "    vis_period          = 0,\n",
    ")\n",
    "# faster_rcnn_101 # model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "register_coco_instances(\"iSAID_train\", {}, \n",
    "                        \"/apps/local/shared/CV703/datasets/iSAID/iSAID_patches/train/instancesonly_filtered_train.json\",\n",
    "                        \"/apps/local/shared/CV703/datasets/iSAID/iSAID_patches/train/images/\")\n",
    "register_coco_instances(\"iSAID_val\", {}, \n",
    "                        \"/apps/local/shared/CV703/datasets/iSAID/iSAID_patches/val/instancesonly_filtered_val.json\",\n",
    "                        \"/apps/local/shared/CV703/datasets/iSAID/iSAID_patches/val/images/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    lr: 0.02\n",
       "    momentum: 0.9\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0001\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = get_default_optimizer_params(\n",
    "    faster_rcnn_101, # model\n",
    "    base_lr             = config.SOLVER.BASE_LR,\n",
    "    weight_decay_norm   = config.SOLVER.WEIGHT_DECAY_NORM,\n",
    "    bias_lr_factor      = config.SOLVER.BIAS_LR_FACTOR,\n",
    "    weight_decay_bias   = config.SOLVER.WEIGHT_DECAY_BIAS,\n",
    ") \n",
    "# ^ this can be expanded further\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr              = config.SOLVER.BASE_LR,\n",
    "    momentum        = config.SOLVER.MOMENTUM,\n",
    "    nesterov        = config.SOLVER.NESTEROV,\n",
    "    weight_decay    = config.SOLVER.WEIGHT_DECAY,\n",
    ")\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<detectron2.data.common.AspectRatioGroupedDataset at 0x7eff4495b050>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_train = True\n",
    "dataset = get_detection_dataset_dicts(\n",
    "    config.DATASETS.TRAIN,\n",
    "    filter_empty    = config.DATALOADER.FILTER_EMPTY_ANNOTATIONS,\n",
    "    min_keypoints   = config.MODEL.ROI_KEYPOINT_HEAD.MIN_KEYPOINTS_PER_IMAGE\n",
    "                    if config.MODEL.KEYPOINT_ON\n",
    "                    else 0,\n",
    "    proposal_files  = config.DATASETS.PROPOSAL_FILES_TRAIN if config.MODEL.LOAD_PROPOSALS else None,\n",
    ")\n",
    "\n",
    "sampler = TrainingSampler(len(dataset))\n",
    "\n",
    "augmentations = [\n",
    "    T.ResizeShortestEdge(\n",
    "        config.INPUT.MIN_SIZE_TRAIN, \n",
    "        config.INPUT.MAX_SIZE_TRAIN, \n",
    "        config.INPUT.MIN_SIZE_TRAIN_SAMPLING\n",
    "    ),\n",
    "    T.RandomFlip(\n",
    "        horizontal  = config.INPUT.RANDOM_FLIP == \"horizontal\",\n",
    "        vertical    = config.INPUT.RANDOM_FLIP == \"vertical\",\n",
    "    ),\n",
    "]\n",
    "if config.INPUT.CROP.ENABLED and is_train:\n",
    "    augmentations.insert(0, T.RandomCrop(config.INPUT.CROP.TYPE, config.INPUT.CROP.SIZE))\n",
    "    recompute_boxes = config.MODEL.MASK_ON\n",
    "else:\n",
    "    recompute_boxes = False\n",
    "\n",
    "mapper = DatasetMapper(\n",
    "    is_train                    = is_train,\n",
    "    augmentations               = augmentations,\n",
    "    image_format                = config.INPUT.FORMAT,\n",
    "    use_instance_mask           = config.MODEL.MASK_ON,\n",
    "    use_keypoint                = config.MODEL.KEYPOINT_ON,\n",
    "    instance_mask_format        = config.INPUT.MASK_FORMAT,\n",
    "    keypoint_hflip_indices      = None,\n",
    "    precomputed_proposal_topk   = None,\n",
    "    recompute_boxes             = recompute_boxes,\n",
    ")\n",
    "\n",
    "if isinstance(dataset, list):\n",
    "    dataset = DatasetFromList(dataset, copy=False)\n",
    "dataset = MapDataset(dataset, mapper)\n",
    "\n",
    "data_loader = build_batch_data_loader(\n",
    "    dataset,\n",
    "    sampler,\n",
    "    total_batch_size        = config.SOLVER.IMS_PER_BATCH,\n",
    "    aspect_ratio_grouping   = config.DATALOADER.ASPECT_RATIO_GROUPING,\n",
    "    num_workers             = config.DATALOADER.NUM_WORKERS,\n",
    "    collate_fn              = None,\n",
    ")\n",
    "data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<detectron2.engine.train_loop.SimpleTrainer at 0x7efeead5f950>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faster_rcnn_101.train()\n",
    "\n",
    "trainer = SimpleTrainer(\n",
    "    faster_rcnn_101, # model\n",
    "    data_loader,\n",
    "    optimizer\n",
    ")\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<detectron2.solver.lr_scheduler.LRMultiplier at 0x7efedf598b90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = [x for x in config.SOLVER.STEPS if x <= config.SOLVER.MAX_ITER]\n",
    "\n",
    "sched = MultiStepParamScheduler(\n",
    "    values      = [config.SOLVER.GAMMA ** k for k in range(len(steps) + 1)],\n",
    "    milestones  = steps,\n",
    "    num_updates = config.SOLVER.MAX_ITER,\n",
    ")\n",
    "\n",
    "sched = WarmupParamScheduler(\n",
    "    sched,\n",
    "    config.SOLVER.WARMUP_FACTOR,\n",
    "    min(config.SOLVER.WARMUP_ITERS / config.SOLVER.MAX_ITER, 1.0),\n",
    "    config.SOLVER.WARMUP_METHOD,\n",
    ")\n",
    "    \n",
    "scheduler = LRMultiplier(\n",
    "    optimizer, \n",
    "    multiplier  = sched, \n",
    "    max_iter    = config.SOLVER.MAX_ITER\n",
    ")\n",
    "scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_head.fc1.weight' to the model due to incompatible shapes: (1024, 12544) in the checkpoint but (1024, 50176) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (16, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (16,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (60, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (60,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_head.fc1.weight\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.fc3.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.fc4.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'__author__': 'Detectron2 Model Zoo'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume = False\n",
    "\n",
    "checkpointer = DetectionCheckpointer(\n",
    "    faster_rcnn_101, # model\n",
    "    config.OUTPUT_DIR,\n",
    "    optimizer   = optimizer,\n",
    "    trainer     = trainer\n",
    ")\n",
    "checkpointer.resume_or_load(config.MODEL.WEIGHTS, resume=resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "get_event_storage() has to be called inside a 'with EventStorage(...)' context!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2373571/1246054358.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaster_rcnn_101\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cv703_project/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cv703_project/lib/python3.7/site-packages/detectron2/modeling/meta_arch/rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproposal_generator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproposal_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_instances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m\"proposals\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatched_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cv703_project/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cv703_project/lib/python3.7/site-packages/detectron2/modeling/proposal_generator/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, gt_instances)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0mgt_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_and_sample_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_instances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             losses = self.losses(\n\u001b[0;32m--> 473\u001b[0;31m                 \u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_objectness_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_anchor_deltas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m             )\n\u001b[1;32m    475\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cv703_project/lib/python3.7/site-packages/detectron2/modeling/proposal_generator/rpn.py\u001b[0m in \u001b[0;36mlosses\u001b[0;34m(self, anchors, pred_objectness_logits, gt_labels, pred_anchor_deltas, gt_boxes)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0mnum_pos_anchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0mnum_neg_anchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgt_labels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_event_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m         \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rpn/num_pos_anchors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_pos_anchors\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rpn/num_neg_anchors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_neg_anchors\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cv703_project/lib/python3.7/site-packages/detectron2/utils/events.py\u001b[0m in \u001b[0;36mget_event_storage\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     assert len(\n\u001b[1;32m     33\u001b[0m         \u001b[0m_CURRENT_STORAGE_STACK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     ), \"get_event_storage() has to be called inside a 'with EventStorage(...)' context!\"\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_CURRENT_STORAGE_STACK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: get_event_storage() has to be called inside a 'with EventStorage(...)' context!"
     ]
    }
   ],
   "source": [
    "curr_iter = 0\n",
    "start_iter = 0\n",
    "max_iter = config.SOLVER.MAX_ITER\n",
    "\n",
    "for i in range(start_iter, 1):\n",
    "    data = next(iter(data_loader))\n",
    "    loss_dict = faster_rcnn_101(data)\n",
    "    print(loss_dict)\n",
    "\n",
    "#     if isinstance(loss_dict, torch.Tensor):\n",
    "#         losses = loss_dict\n",
    "#         loss_dict = {\"total_loss\": loss_dict}\n",
    "#     else:\n",
    "#         losses = sum(loss_dict.values())\n",
    "\n",
    "#     optimizer.zero_grad()\n",
    "#     losses.backward()\n",
    "\n",
    "#     optimizer.step()\n",
    "# curr_iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dc9b69e64368548486b00be2f71235e2e56e83a6846660d4371ccec56b5c4e06"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('cv703_project': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}